{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2752be0e",
   "metadata": {},
   "source": [
    "# 1. Start a ChatBot session to understand what a Classification Decision Tree is: (a) ask the ChatBot to describe the type of problem a Classification Decision Tree addresses and provide some examples of real-world applications where this might be particularly useful, and then (b) make sure you understand the difference between how a Classification Decision Tree makes (classification) predictions versus how Multiple Linear Regression makes (regression) predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b372b6f8",
   "metadata": {},
   "source": [
    "a) A Classification Decision Tree is a type of supervised machine learning algorithm designed to solve classification problems, where the goal is to assign a category (or class) label to each data point. It works by recursively splitting the dataset into subsets based on the value of a specific feature, creating a tree structure. Each internal node represents a decision based on a feature, each branch represents an outcome of the decision, and each leaf node represents a class label.\n",
    "b) Key Differences:\n",
    "- Output Type:\n",
    "1. Decision Tree: Discrete (class labels).\n",
    "2. Regression: Continuous (numerical values).\n",
    "- Prediction Mechanism:\n",
    "1. Decision Tree: Sequence of decisions based on feature values.\n",
    "2. Regression: Weighted linear combination of features.\n",
    "- Applicability:\n",
    "1. Decision Tree: Classification problems (e.g., spam detection).\n",
    "2. Regression: Regression problems (e.g., stock price prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0962ce21",
   "metadata": {},
   "source": [
    "# 2. Continue your ChatBot session and explore with your ChatBot what real-world application scenario(s) might be most appropriately addressed by each of the following metrics below: provide your answers and, in your own words, concisely explain your rationale for your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c703a",
   "metadata": {},
   "source": [
    "### 1.Accuracy measures the proportion of true results (both true positives and true negatives) in the population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbecea5",
   "metadata": {},
   "source": [
    "Scenario: General diagnostics in stable populations with balanced classes.\n",
    "My explanation: when we have stable populations and balanced classes, we have very similar false positives and false negatives that eventually balance out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a5e9d6",
   "metadata": {},
   "source": [
    "### 2.Sensitivity measures the proportion of actual positives that are correctly identified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2240fc25",
   "metadata": {},
   "source": [
    "Scenario: Medical screening for critical conditions where missing a positive case is costly.\n",
    "My explanation: when false negatives give very big outcomes that are rather considered bad, sensitivity really helps in these situations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a581c4",
   "metadata": {},
   "source": [
    "### 3.Specificity measures the proportion of actual negatives that are correctly identified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6096f0fb",
   "metadata": {},
   "source": [
    "Scenario: Preventing false alarms in security systems or diagnostics where over-detection causes unnecessary action.\n",
    "My explanation: when false positives lead to more mental workload, specificity plays a big role."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292123e6",
   "metadata": {},
   "source": [
    "### 4.Precision measures the proportion of positive identifications that were actually correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e956c538",
   "metadata": {},
   "source": [
    "Scenario: High-stakes decision-making where false positives must be minimized.\n",
    "My explanation: when the cost of false positive is higher than the cost of false negatives, we need precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b97dcd7",
   "metadata": {},
   "source": [
    "# 3. Explore the amazon books dataset, seen previously at the start of the semester, providing some initital standard exploratory data analysis (EDA) and data summarization after pre-processing the dataset to meet the requirements below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b034e",
   "metadata": {},
   "source": [
    "1. remove Weight_oz, Width, and Height\n",
    "2. drop all remaining rows with NaN entries\n",
    "3. set Pub year and NumPages to have the type int, and Hard_or_Paper to have the type category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6407868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, recall_score, make_scorer\n",
    "import graphviz as gv\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/pointOfive/STA130_F23/main/Data/amazonbooks.csv\"\n",
    "ab = pd.read_csv(url, encoding=\"ISO-8859-1\")\n",
    "# create `ab_reduced_noNaN` based on the specs above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0e2698",
   "metadata": {},
   "source": [
    "Here is the code for doing all that was asked to the previous dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd48a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"amazonbooks.csv\"  # Ensure the file is in the same directory as your Jupyter notebook\n",
    "ab = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Pre-process the dataset based on the requirements\n",
    "# Remove specified columns: Weight_oz, Width, and Height\n",
    "ab_reduced = ab.drop(columns=[\"Weight_oz\", \"Width\", \"Height\"])\n",
    "\n",
    "# Drop all remaining rows with NaN entries\n",
    "ab_reduced_noNaN = ab_reduced.dropna()\n",
    "\n",
    "# Set Pub year and NumPages to type int, and Hard_or_Paper to type category\n",
    "ab_reduced_noNaN[\"Pub year\"] = ab_reduced_noNaN[\"Pub year\"].astype(int)\n",
    "ab_reduced_noNaN[\"NumPages\"] = ab_reduced_noNaN[\"NumPages\"].astype(int)\n",
    "ab_reduced_noNaN[\"Hard_or_Paper\"] = ab_reduced_noNaN[\"Hard_or_Paper\"].astype(\"category\")\n",
    "\n",
    "# Display a summary of the dataset\n",
    "print(\"Dataset Shape:\", ab_reduced_noNaN.shape)\n",
    "print(\"\\nColumn Types:\\n\", ab_reduced_noNaN.dtypes)\n",
    "print(\"\\nFirst Few Rows:\\n\", ab_reduced_noNaN.head())\n",
    "print(\"\\nStatistical Summary:\\n\", ab_reduced_noNaN.describe(include=\"all\"))\n",
    "\n",
    "# Save the processed dataset to a new file if needed\n",
    "# ab_reduced_noNaN.to_csv(\"processed_amazonbooks.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06614ccf",
   "metadata": {},
   "source": [
    "# 4. Create an 80/20 split with 80% of the data as a training set ab_reduced_noNaN_train and 20% of the data testing set ab_reduced_noNaN_test using either df.sample(...) as done in TUT or using train_test_split(...) as done in the previous HW, and report on how many observations there are in the training data set and the test data set.\n",
    "\n",
    "# Tell a ChatBot that you are about to fit a \"scikit-learn\" DecisionTreeClassifier model and ask what the two steps given below are doing; then use your ChatBots help to write code to \"train\" a classification tree clf using only the List Price variable to predict whether or not a book is a hard cover or paper back book using a max_depth of 2; finally use tree.plot_tree(clf) to explain what predictions are made based on List Price for the fitted clf model\n",
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']\n",
    "X = ab_reduced_noNaN[['List Price']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea3c485",
   "metadata": {},
   "source": [
    "(1) Splitting the Data into Training and Testing Sets\n",
    "You can create an 80/20 split using either df.sample() or train_test_split() from scikit-learn. Here‚Äôs the code for splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624d07f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the predictors (X) and the target variable (y)\n",
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']\n",
    "X = ab_reduced_noNaN[['List Price']]\n",
    "\n",
    "# Perform an 80/20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Report the number of observations in each set\n",
    "print(\"Number of observations in the training set:\", len(X_train))\n",
    "print(\"Number of observations in the testing set:\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa79f595",
   "metadata": {},
   "source": [
    "(2) Steps in Fitting a Decision Tree Classifier\n",
    "Here are the two steps typically involved:\n",
    "\n",
    "Step 1 - \"Training\" the Model:\n",
    "This step uses the fit method of the DecisionTreeClassifier. The model learns patterns from the training data ùëã train(predictor variables) and ùë¶ train(target variable). It creates a tree by recursively splitting the data based on feature values to minimize classification error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43243d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39cc99c",
   "metadata": {},
   "source": [
    "Step 2 - \"Making Predictions\":\n",
    "This step uses the predict method to apply the trained model to new, unseen data (e.g., ùëã test). The predictions are the class labels inferred by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c59091",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e157679a",
   "metadata": {},
   "source": [
    "(3) Training a Decision Tree Classifier\n",
    "Here‚Äôs the code to train a classification tree using List Price to predict whether a book is hardcover (Hard_or_Paper = 'H') with a max_depth of 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a90292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the DecisionTreeClassifier with max_depth=2\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Visualize the tree\n",
    "plt.figure(figsize=(12, 8))\n",
    "tree.plot_tree(clf, feature_names=[\"List Price\"], class_names=[\"Paper\", \"Hard\"], filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2fa2cd",
   "metadata": {},
   "source": [
    "(4) Interpreting the Tree\n",
    "The tree visualization provides the following insights:\n",
    "\n",
    "Root Node: Splits the data based on List Price at a certain threshold. For example, if List Price <= $20, the books may tend to be paperback, and otherwise hardcover.\n",
    "Leaf Nodes: Indicate the predicted class (paperback or hardcover) and the proportion of training samples classified at each leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3601f6",
   "metadata": {},
   "source": [
    "# 5. Repeat the previous problem but this time visualize the classification decision tree based on the following specifications below; then explain generally how predictions are made for the clf2 model\n",
    "X = ab_reduced_noNaN[['NumPages', 'Thick', 'List Price']]\n",
    "max_depth set to 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d99b6f",
   "metadata": {},
   "source": [
    "Step 1: Define the Predictors and Target Variable\n",
    "In this problem, the predictors will include NumPages, Thick, and List Price. The target remains whether the book is hardcover (Hard_or_Paper = 'H')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053db3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predictors and target variable\n",
    "X = ab_reduced_noNaN[['NumPages', 'Thick', 'List Price']]\n",
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571abfd7",
   "metadata": {},
   "source": [
    "Step 2: Split the Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff61e1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an 80/20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e0ecf8",
   "metadata": {},
   "source": [
    "Step 3: Train the Classification Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98003964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DecisionTreeClassifier with max_depth=4\n",
    "clf2 = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "clf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9962a89f",
   "metadata": {},
   "source": [
    "Step 4: Visualize the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60046f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tree\n",
    "plt.figure(figsize=(16, 10))\n",
    "tree.plot_tree(clf2, feature_names=['NumPages', 'Thick', 'List Price'], \n",
    "               class_names=[\"Paper\", \"Hard\"], filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecec9b7",
   "metadata": {},
   "source": [
    "General Explanation of Predictions for clf2\n",
    "1. Feature Splits: The tree splits the data based on the most important features (e.g., NumPages, Thick, List Price), optimizing for classification accuracy at each level.\n",
    "2. Decision Path: Each data point traverses the tree, starting from the root node and following the feature-based decision rules until it reaches a leaf node.\n",
    "3. Leaf Nodes: At the leaf nodes, the model makes predictions based on the majority class of the training samples that fall into that node. For example:\n",
    "- If most books in a leaf node are hardcovers, the model predicts \"Hard.\"\n",
    "- If most are paperbacks, it predicts \"Paper.\"\n",
    "The additional depth (max_depth=4) allows the tree to consider more complex relationships among the predictors, potentially leading to more nuanced predictions but with a risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8489c4b4",
   "metadata": {},
   "source": [
    "# 6. Use previously created ab_reduced_noNaN_test to create confusion matrices for clf and clf2. Report the sensitivity, specificity and accuracy for each of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42605cb",
   "metadata": {},
   "source": [
    "1. Recreate the models:\n",
    "\n",
    "- Train clf using List Price only.\n",
    "- Train clf2 using NumPages, Thick, and List Price.\n",
    "\n",
    "2. Redo predictions:\n",
    "\n",
    "- Generate predictions for both clf and clf2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aedb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Retrain the models\n",
    "clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "clf.fit(X_train_list_price, y_train)\n",
    "\n",
    "clf2 = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "clf2.fit(X_train_all_features, y_train)\n",
    "\n",
    "# Generate predictions for the test set using both models\n",
    "y_pred_clf = clf.predict(X_test_list_price)\n",
    "y_pred_clf2 = clf2.predict(X_test_all_features)\n",
    "\n",
    "# Create confusion matrices for both models\n",
    "conf_matrix_clf = confusion_matrix(y_test, y_pred_clf)\n",
    "conf_matrix_clf2 = confusion_matrix(y_test, y_pred_clf2)\n",
    "\n",
    "# Calculate metrics for clf\n",
    "accuracy_clf = accuracy_score(y_test, y_pred_clf)\n",
    "sensitivity_clf = recall_score(y_test, y_pred_clf)  # Sensitivity = True Positive Rate\n",
    "specificity_clf = conf_matrix_clf[0, 0] / (conf_matrix_clf[0, 0] + conf_matrix_clf[0, 1])  # TN / (TN + FP)\n",
    "\n",
    "# Calculate metrics for clf2\n",
    "accuracy_clf2 = accuracy_score(y_test, y_pred_clf2)\n",
    "sensitivity_clf2 = recall_score(y_test, y_pred_clf2)  # Sensitivity = True Positive Rate\n",
    "specificity_clf2 = conf_matrix_clf2[0, 0] / (conf_matrix_clf2[0, 0] + conf_matrix_clf2[0, 1])  # TN / (TN + FP)\n",
    "\n",
    "# Summarize results\n",
    "metrics_summary = {\n",
    "    \"Model\": [\"clf (List Price)\", \"clf2 (All Features)\"],\n",
    "    \"Accuracy\": [accuracy_clf, accuracy_clf2],\n",
    "    \"Sensitivity\": [sensitivity_clf, sensitivity_clf2],\n",
    "    \"Specificity\": [specificity_clf, specificity_clf2]\n",
    "}\n",
    "\n",
    "# Convert to a DataFrame for better readability\n",
    "metrics_summary_df = pd.DataFrame(metrics_summary)\n",
    "\n",
    "# Display the summary\n",
    "print(metrics_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f258a273",
   "metadata": {},
   "source": [
    "# 7. Explain in three to four sentences what is causing the differences between the following two confusion matrices below, and why the two confusion matrices above (for clf and clf2) are better\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix(ab_reduced_noNaN_train.your_actual_outcome_variable, \n",
    "                     clf.predict(ab_reduced_noNaN_train[['List Price']]), \n",
    "                     labels=[0, 1]), display_labels=[\"Paper\",\"Hard\"]).plot()\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix(ab_reduced_noNaN_train.your_actual_outcome_variable, \n",
    "                     clf.predict(\n",
    "                         ab_reduced_noNaN_train[['NumPages','Thick','List Price']]), \n",
    "                     labels=[0, 1]), display_labels=[\"Paper\",\"Hard\"]).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb266bf5",
   "metadata": {},
   "source": [
    "The differences between the confusion matrices arise from the features used by the models. The first matrix (using List Price only) relies on limited information, leading to potentially higher misclassification rates. The second matrix (using NumPages, Thick, and List Price) incorporates more features, capturing complex relationships and improving prediction accuracy. The test set confusion matrices for clf and clf2 are more reliable as they evaluate generalization to unseen data, while training matrices often show overly optimistic results due to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dec08aa",
   "metadata": {},
   "source": [
    "# 8. Read the paragraphs in Further Guidance and ask a ChatBot how to visualize feature Importances available for scikit-learn classification decision trees; do so for clf2; and use .feature_names_in_ corresponding to .feature_importances_ to report which predictor variable is most important for making predictions according to clf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464d34db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract feature importances and corresponding feature names\n",
    "importances = clf2.feature_importances_\n",
    "feature_names = clf2.feature_names_in_\n",
    "\n",
    "# Create a bar chart for visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(feature_names, importances, color=\"skyblue\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importances for clf2\")\n",
    "plt.show()\n",
    "\n",
    "# Report the most important predictor variable\n",
    "most_important_feature = feature_names[importances.argmax()]\n",
    "print(f\"The most important predictor variable is: {most_important_feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a39db5",
   "metadata": {},
   "source": [
    "The feature_importances_ values represent how much each feature contributes to reducing impurity in the tree's splits. Higher values indicate more significant predictors for the classification task. After running this code, the bar chart will highlight the relative importance of NumPages, Thick, and List Price in clf2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addf24ad",
   "metadata": {},
   "source": [
    "# 9. Describe the differences of interpreting coefficients in linear model regression versus feature importances in decision trees in two to three sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78619fce",
   "metadata": {},
   "source": [
    "In linear regression models, coefficients represent the direct effect of each predictor on the target variable, assuming all other predictors are held constant; they provide insight into the magnitude and direction of the relationship (positive or negative). In decision trees, feature importances indicate how much each feature contributes to reducing impurity (e.g., Gini index or entropy) across all splits, without providing a specific numerical relationship or direction. Unlike regression coefficients, feature importances in trees are relative, additive, and less interpretable for understanding linear or causal effects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
